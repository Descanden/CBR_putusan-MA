{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8f9f0e",
   "metadata": {},
   "source": [
    "---\n",
    "# Perbandingan Pendekatan Case-Based Reasoning Berbasis TF-IDF: Cosine Similarity vs. SVM dalam Analisis Putusan Perdata PMH\n",
    "\n",
    "Anggota Kelompok :\n",
    "\n",
    "1. Rofiq Samanhudi - 202210370311260\n",
    "\n",
    "2. Muhammad Ikbar Ananda Sulistio - 202210370311236\n",
    "\n",
    "## Tugas Besar Mata Kuliah Penalaran Komputer (B)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab23841",
   "metadata": {},
   "source": [
    "## Tahap 1 – Membangun Case Base\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk mengumpulkan dan mempersiapkan korpus dokumen putusan pengadilan dalam bentuk teks yang bersih dan terstruktur. Dokumen ini akan menjadi basis bagi seluruh proses reasoning pada sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Seleksi dan Pengunduhan Dokumen\n",
    "\n",
    "- Pemilihan jenis perkara: **Perdata - Perbuatan Melawan Hukum (PMH)**.\n",
    "- Proses scraping dilakukan dari situs putusan Mahkamah Agung.\n",
    "- File unduhan disimpan dalam bentuk PDF di direktori: `PMH_PDF/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Konversi dan Ekstraksi Teks\n",
    "\n",
    "- File PDF dikonversi menjadi teks menggunakan library `pdfminer`.\n",
    "- Setiap file diproses untuk mengambil seluruh isi putusan sebagai teks mentah.\n",
    "- Output teks disimpan per dokumen di folder: `data/raw/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Pembersihan Teks (Cleaning)\n",
    "\n",
    "- Menghapus watermark, header, footer, dan disclaimer resmi dari Mahkamah Agung.\n",
    "- Normalisasi teks dilakukan dengan:\n",
    "  - Menghilangkan baris kosong ganda.\n",
    "  - Mengubah huruf menjadi lowercase.\n",
    "  - Menghapus spasi berlebih.\n",
    "- Rasio retensi konten dihitung untuk mengevaluasi hasil pembersihan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Validasi dan Logging\n",
    "\n",
    "- Setiap file yang berhasil diproses dilaporkan ke dalam log:\n",
    "  - Nomor dokumen\n",
    "  - Status pembersihan (OK atau rendah)\n",
    "  - Lokasi penyimpanan\n",
    "  - Tanggal dan waktu proses\n",
    "- Log disimpan di file: `logs/cleaning.log`.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Tahap Ini\n",
    "\n",
    "- Total dokumen yang diproses: **47 kasus**.\n",
    "- Dokumen teks bersih disimpan di: `data/raw/`\n",
    "- Log proses dan status pembersihan: `logs/cleaning.log`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap pertama ini berhasil membentuk korpus awal untuk sistem CBR. Dokumen telah dikonversi ke format teks dan dibersihkan secara sistematis, siap untuk tahap representasi selanjutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7913038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning PDF text extraction...\n",
      "[OK] case_001 processed (96.35% valid).\n",
      "[OK] case_002 processed (96.25% valid).\n",
      "[OK] case_003 processed (96.36% valid).\n",
      "[OK] case_004 processed (96.48% valid).\n",
      "[OK] case_005 processed (96.34% valid).\n",
      "[OK] case_006 processed (96.38% valid).\n",
      "[OK] case_007 processed (96.10% valid).\n",
      "[OK] case_008 processed (96.30% valid).\n",
      "[OK] case_009 processed (96.43% valid).\n",
      "[OK] case_010 processed (96.40% valid).\n",
      "[OK] case_011 processed (96.42% valid).\n",
      "[OK] case_012 processed (96.45% valid).\n",
      "[OK] case_013 processed (95.81% valid).\n",
      "[OK] case_014 processed (96.56% valid).\n",
      "[OK] case_015 processed (96.24% valid).\n",
      "[OK] case_016 processed (96.26% valid).\n",
      "[OK] case_017 processed (96.22% valid).\n",
      "[OK] case_018 processed (96.21% valid).\n",
      "[OK] case_019 processed (96.46% valid).\n",
      "[OK] case_020 processed (96.71% valid).\n",
      "[OK] case_021 processed (96.55% valid).\n",
      "[OK] case_022 processed (96.23% valid).\n",
      "[OK] case_023 processed (96.41% valid).\n",
      "[OK] case_024 processed (96.18% valid).\n",
      "[OK] case_025 processed (96.45% valid).\n",
      "[OK] case_026 processed (96.25% valid).\n",
      "[OK] case_027 processed (96.33% valid).\n",
      "[OK] case_028 processed (96.26% valid).\n",
      "[OK] case_029 processed (96.01% valid).\n",
      "[OK] case_030 processed (96.23% valid).\n",
      "[OK] case_031 processed (96.22% valid).\n",
      "[OK] case_032 processed (96.13% valid).\n",
      "[OK] case_033 processed (96.38% valid).\n",
      "[OK] case_034 processed (96.19% valid).\n",
      "[OK] case_035 processed (96.17% valid).\n",
      "[OK] case_036 processed (96.12% valid).\n",
      "[OK] case_037 processed (96.19% valid).\n",
      "[OK] case_038 processed (96.31% valid).\n",
      "[OK] case_039 processed (96.24% valid).\n",
      "[OK] case_040 processed (96.24% valid).\n",
      "[OK] case_041 processed (96.13% valid).\n",
      "[OK] case_042 processed (96.49% valid).\n",
      "[OK] case_043 processed (96.43% valid).\n",
      "[OK] case_044 processed (96.25% valid).\n",
      "[OK] case_045 processed (96.35% valid).\n",
      "[OK] case_046 processed (96.42% valid).\n",
      "[OK] case_047 processed (96.32% valid).\n",
      "[OK] case_048 processed (96.58% valid).\n",
      "[OK] case_049 processed (96.35% valid).\n",
      "[OK] case_050 processed (96.28% valid).\n",
      "[OK] case_051 processed (96.55% valid).\n",
      "[OK] case_052 processed (96.33% valid).\n",
      "[OK] case_053 processed (96.34% valid).\n",
      "[OK] case_054 processed (96.52% valid).\n",
      "[OK] case_055 processed (96.47% valid).\n",
      "[OK] case_056 processed (96.50% valid).\n",
      "[OK] case_057 processed (96.34% valid).\n",
      "[OK] case_058 processed (96.49% valid).\n",
      "[OK] case_059 processed (96.12% valid).\n",
      "[OK] case_060 processed (96.22% valid).\n",
      "[OK] case_061 processed (96.33% valid).\n",
      "[OK] case_062 processed (96.62% valid).\n",
      "[OK] case_063 processed (96.37% valid).\n",
      "[OK] case_064 processed (96.24% valid).\n",
      "[OK] case_065 processed (96.47% valid).\n",
      "[OK] case_066 processed (96.21% valid).\n",
      "[OK] case_067 processed (96.41% valid).\n",
      "[OK] case_068 processed (96.38% valid).\n",
      "[OK] case_069 processed (96.50% valid).\n",
      "[OK] case_070 processed (96.57% valid).\n",
      "PDF processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from pdfminer.high_level import extract_text\n",
    "from datetime import datetime\n",
    "\n",
    "# === Directory configuration ===\n",
    "SOURCE_DIR = 'PMH_PDF'\n",
    "DEST_DIR = 'data/raw'\n",
    "LOG_DEST = 'logs/cleaning.log'\n",
    "\n",
    "def purify_text(input_text):\n",
    "    \"\"\"Remove unwanted headers, footers, and disclaimers from extracted text.\"\"\"\n",
    "    initial_length = len(input_text) if input_text else 1  # Prevent division by zero\n",
    "\n",
    "    # Eliminate watermarks and page indicators\n",
    "    processed_text = input_text\n",
    "    processed_text = processed_text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    processed_text = processed_text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    processed_text = re.sub(r'halaman\\s*\\d+', '', processed_text, flags=re.IGNORECASE)\n",
    "    processed_text = processed_text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    \n",
    "    # Record length after initial cleaning\n",
    "    intermediate_length = len(processed_text)\n",
    "    \n",
    "    # Remove disclaimer content\n",
    "    processed_text = processed_text.replace(\"Disclaimer\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\")\n",
    "\n",
    "    # Final text normalization\n",
    "    processed_text = processed_text.lower()\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "\n",
    "    # Calculate content retention ratio\n",
    "    retention_ratio = intermediate_length / initial_length\n",
    "\n",
    "    return processed_text, retention_ratio\n",
    "\n",
    "def record_log(case_id, retention_ratio):\n",
    "    \"\"\"Log processing details to a file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(LOG_DEST), exist_ok=True)\n",
    "    with open(LOG_DEST, 'a', encoding='utf-8') as log:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        log.write(f\"[{timestamp}] {case_id} | Content Integrity: {retention_ratio:.2%}\\n\")\n",
    "\n",
    "def handle_pdf_batch():\n",
    "    \"\"\"Process all PDFs in the source directory.\"\"\"\n",
    "    os.makedirs(DEST_DIR, exist_ok=True)\n",
    "    pdf_paths = sorted(glob.glob(os.path.join(SOURCE_DIR, '*.pdf')))\n",
    "    \n",
    "    if not pdf_paths:\n",
    "        print(\"No PDFs found in the source directory.\")\n",
    "        return\n",
    "\n",
    "    for index, pdf in enumerate(pdf_paths, 1):\n",
    "        case_id = f\"case_{index:03d}\"\n",
    "        output_path = os.path.join(DEST_DIR, f\"{case_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            raw_text = extract_text(pdf)\n",
    "\n",
    "            # Clean and validate text\n",
    "            final_text, integrity_ratio = purify_text(raw_text)\n",
    "\n",
    "            # Check content integrity\n",
    "            if integrity_ratio < 0.8:\n",
    "                print(f\"[WARNING] {case_id} retains only {integrity_ratio:.2%} of content.\")\n",
    "            else:\n",
    "                print(f\"[OK] {case_id} processed ({integrity_ratio:.2%} valid).\")\n",
    "                # Save cleaned text if valid\n",
    "                with open(output_path, 'w', encoding='utf-8') as output:\n",
    "                    output.write(final_text)\n",
    "\n",
    "            # Log the result\n",
    "            record_log(case_id, integrity_ratio)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"[ERROR] Failed to process {pdf}: {str(err)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Initiate PDF processing workflow.\"\"\"\n",
    "    print(\"Beginning PDF text extraction...\")\n",
    "    handle_pdf_batch()\n",
    "    print(\"PDF processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645feb1c",
   "metadata": {},
   "source": [
    "## Tahap 2 – Case Representation\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk merepresentasikan setiap putusan dalam struktur data yang terorganisir. Representasi ini akan menjadi basis data terstruktur yang siap digunakan untuk proses retrieval dan analisis pada sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Ekstraksi Metadata\n",
    "\n",
    "Setiap dokumen hasil pembersihan teks dianalisis untuk mengekstrak informasi penting sebagai metadata menggunakan pendekatan berbasis pola (regex) dan pemrosesan teks. Metadata yang diekstrak meliputi:\n",
    "\n",
    "- Nomor Perkara (`no_perkara`)\n",
    "- Tanggal Putusan (`tanggal`)\n",
    "- Ringkasan Fakta (`ringkasan_fakta`)\n",
    "- Pasal yang digunakan atau dasar hukum (`pasal`)\n",
    "- Pihak-pihak yang terlibat (tergugat dan penggugat)\n",
    "- Isi teks lengkap dari putusan (`text_full`)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Penyimpanan Data Terstruktur\n",
    "\n",
    "Hasil ekstraksi metadata disimpan dalam dua format file untuk kemudahan pemrosesan:\n",
    "\n",
    "- **CSV** → `data/processed/cases_extracted.csv`\n",
    "- **JSON** → `data/processed/cases_extracted.json`\n",
    "\n",
    "Struktur data untuk masing-masing kasus mencakup kolom:\n",
    "- `case_id`\n",
    "- `no_perkara`\n",
    "- `tanggal`\n",
    "- `ringkasan_fakta`\n",
    "- `pasal`\n",
    "- `pihak`\n",
    "- `text_full`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Feature Engineering\n",
    "\n",
    "Untuk meningkatkan kualitas representasi kasus, dilakukan proses rekayasa fitur (feature engineering) sebagai berikut:\n",
    "\n",
    "- **Panjang Teks (Length)**: Menghitung jumlah kata dalam `ringkasan_fakta`.\n",
    "- **Bag-of-Words (BoW)**: Menghitung frekuensi kata yang muncul dalam ringkasan.\n",
    "- **QA-Pairs Sederhana**: Menghasilkan pasangan pertanyaan dan jawaban berbasis konten ringkasan, misalnya:\n",
    "  - Apa nomor perkaranya?\n",
    "  - Apa pasal yang relevan?\n",
    "  - Siapa pihak tergugat?\n",
    "  - Apa ringkasan faktanya?\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Penyimpanan Fitur\n",
    "\n",
    "Fitur-fitur yang telah direkayasa disimpan dalam file JSON untuk fleksibilitas dalam analisis selanjutnya:\n",
    "\n",
    "- `data/processed/features_length.json`\n",
    "- `data/processed/features_bow.json`\n",
    "- `data/processed/features_qa_pairs.json`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap representasi berhasil membentuk struktur data yang terorganisir untuk semua kasus yang tersedia. Masing-masing kasus telah dilengkapi dengan metadata penting dan fitur tambahan untuk mendukung proses retrieval dan reasoning pada tahapan berikutnya dalam sistem CBR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4676e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] 70 kasus disimpan ke:\n",
      "CSV → data/processed\\cases_extracted.csv\n",
      "JSON → data/processed\\cases_extracted.json\n",
      "Length → data/processed\\features_length.json\n",
      "BoW → data/processed\\features_bow.json\n",
      "QA Pairs → data/processed\\features_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "RAW_FOLDER = 'data/raw'\n",
    "PROCESSED_FOLDER = 'data/processed'\n",
    "\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_metadata(text):\n",
    "    metadata = {}\n",
    "\n",
    "    # Nomor Perkara\n",
    "    match = re.search(r'Nomor\\s*:\\s*(\\S+)', text, re.IGNORECASE)\n",
    "    metadata['no_perkara'] = match.group(1) if match else ''\n",
    "\n",
    "    # Tanggal\n",
    "    match = re.search(r'\\b(?:putusan|diputuskan) (?:pada|tanggal)?\\s*(\\d{1,2}\\s+\\w+\\s+\\d{4})', text, re.IGNORECASE)\n",
    "    metadata['tanggal'] = match.group(1) if match else ''\n",
    "\n",
    "    # Pasal\n",
    "    match = re.findall(r'Pasal\\s+\\d+\\s+[^\\n.,;]*', text, re.IGNORECASE)\n",
    "    metadata['pasal'] = match if match else []\n",
    "\n",
    "    # Ringkasan Fakta\n",
    "    match = re.search(r'(?:menimbang\\s+bahwa.*?)((?=menimbang\\s+bahwa|mengingat\\b|memperhatikan\\b).{0,300})', text, re.IGNORECASE | re.DOTALL)\n",
    "    metadata['ringkasan_fakta'] = match.group(1).strip() if match else ''\n",
    "\n",
    "    # Terdakwa & Korban\n",
    "    terdakwa = re.findall(r'terdakwa(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    korban = re.findall(r'korban(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    metadata['pihak'] = {\n",
    "        'terdakwa': terdakwa[0] if terdakwa else '',\n",
    "        'korban': korban[0] if korban else ''\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def feature_engineering(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    length = len(tokens)\n",
    "    bow = dict(Counter(tokens))\n",
    "    \n",
    "    qa_pairs = {\n",
    "        \"Apa nomor perkaranya?\": \"\",\n",
    "        \"Apa pasal yang dilanggar?\": \"\",\n",
    "        \"Siapa terdakwanya?\": \"\",\n",
    "        \"Siapa korbannya?\": \"\"\n",
    "    }\n",
    "\n",
    "    return length, bow, qa_pairs\n",
    "\n",
    "def process_all():\n",
    "    all_cases = []\n",
    "    features_length = {}\n",
    "    features_bow = {}\n",
    "    features_qa = {}\n",
    "\n",
    "    txt_files = sorted([f for f in os.listdir(RAW_FOLDER) if f.endswith('.txt')])\n",
    "    for i, fname in enumerate(txt_files, 1):\n",
    "        path = os.path.join(RAW_FOLDER, fname)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        case_id = f\"case_{i:03d}\"\n",
    "        metadata = extract_metadata(text)\n",
    "        metadata['case_id'] = case_id\n",
    "        metadata['text_full'] = text\n",
    "\n",
    "        length, bow, qa = feature_engineering(text)\n",
    "        qa[\"Apa nomor perkaranya?\"] = metadata.get(\"no_perkara\", \"\")\n",
    "        qa[\"Apa pasal yang dilanggar?\"] = \", \".join(metadata.get(\"pasal\", []))\n",
    "        qa[\"Siapa terdakwanya?\"] = metadata.get(\"pihak\", {}).get(\"terdakwa\", \"\")\n",
    "        qa[\"Siapa korbannya?\"] = metadata.get(\"pihak\", {}).get(\"korban\", \"\")\n",
    "\n",
    "        all_cases.append(metadata)\n",
    "        features_length[case_id] = length\n",
    "        features_bow[case_id] = bow\n",
    "        features_qa[case_id] = qa\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.csv')\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['case_id', 'no_perkara', 'tanggal', 'ringkasan_fakta', 'pasal', 'pihak', 'text_full']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_cases:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Save JSON\n",
    "    json_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save features\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_length.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_length, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_bow.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_bow, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_qa, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[SUKSES] {len(all_cases)} kasus disimpan ke:\")\n",
    "    print(\"CSV →\", csv_path)\n",
    "    print(\"JSON →\", json_path)\n",
    "    print(\"Length →\", os.path.join(PROCESSED_FOLDER, 'features_length.json'))\n",
    "    print(\"BoW →\", os.path.join(PROCESSED_FOLDER, 'features_bow.json'))\n",
    "    print(\"QA Pairs →\", os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ef46e",
   "metadata": {},
   "source": [
    "## Tahap 3 – Case Retrieval\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menemukan kasus-kasus lama yang paling relevan dan mirip dengan kasus baru (query) yang diajukan. Proses ini merupakan bagian inti dari sistem Case-Based Reasoning (CBR), yang mendukung pencarian preseden hukum berdasarkan kemiripan konten.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Representasi Vektor\n",
    "\n",
    "Setiap ringkasan fakta dari putusan diubah menjadi representasi vektor menggunakan algoritma **TF-IDF (Term Frequency – Inverse Document Frequency)**. Representasi ini mengubah teks menjadi format numerik agar bisa digunakan untuk penghitungan kemiripan dan klasifikasi.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Splitting Data\n",
    "\n",
    "Dataset dibagi menjadi dua bagian:\n",
    "- **Data latih (training set)** sebanyak 80%\n",
    "- **Data uji (test set)** sebanyak 20%\n",
    "\n",
    "Splitting ini digunakan untuk pelatihan model klasifikasi berbasis TF-IDF + SVM, serta evaluasi awal performa retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Model Retrieval\n",
    "\n",
    "Dua pendekatan digunakan untuk proses retrieval:\n",
    "\n",
    "- **a. TF-IDF + Cosine Similarity (CBR Approach)**  \n",
    "  Menggunakan cosine similarity untuk mengukur kemiripan vektor TF-IDF antara query dan semua kasus dalam basis data. Top-k kasus dengan skor tertinggi dianggap sebagai yang paling relevan.\n",
    "\n",
    "- **b. TF-IDF + Support Vector Machine (SVM)**  \n",
    "  Menggunakan pendekatan supervised learning. Model SVM dilatih untuk memetakan ringkasan fakta ke `case_id`. Diberikan sebuah query, model akan memprediksi case_id yang paling sesuai.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Fungsi Retrieval\n",
    "\n",
    "Fungsi retrieval disiapkan untuk masing-masing pendekatan:\n",
    "\n",
    "- `retrieve_cosine(query: str, k: int = 5)`  \n",
    "  Mengembalikan top-k `case_id` dengan skor cosine tertinggi dari seluruh basis data.\n",
    "\n",
    "- `retrieve_svm(query: str)`  \n",
    "  Mengembalikan satu `case_id` yang diprediksi oleh model SVM berdasarkan klasifikasi.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Pengujian Awal\n",
    "\n",
    "Sebanyak 10 query uji disiapkan beserta ground-truth case ID. Untuk masing-masing query:\n",
    "- Dihitung top-5 hasil cosine similarity.\n",
    "- Diprediksi top-1 hasil dari model SVM.\n",
    "- Hasilnya dibandingkan dengan ground-truth.\n",
    "\n",
    "Query uji disimpan di:\n",
    "- `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "File hasil yang dihasilkan dari tahap ini meliputi:\n",
    "- Model klasifikasi (SVM): `03_retrieval_model.pkl`\n",
    "- TF-IDF Vectorizer: `03_vectorizer.pkl`\n",
    "- Dataset query uji: `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap Case Retrieval telah berhasil diimplementasikan menggunakan dua pendekatan: unsupervised berbasis kemiripan teks dan supervised berbasis klasifikasi. Keduanya siap digunakan untuk evaluasi dan prediksi solusi dalam tahapan berikutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e920067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Model disimpan di : 03_retrieval_model.pkl\n",
      "[SUKSES] Vectorizer disimpan di : 03_vectorizer.pkl\n",
      "[SUKSES] 10 query uji disimpan di : data/eval/queries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# === Konfigurasi path ===\n",
    "CSV_PATH = \"data/processed/cases_extracted.csv\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === Muat data CSV ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "texts = df['ringkasan_fakta'].fillna(df['text_full']).astype(str)\n",
    "labels = df['case_id'].astype(str)\n",
    "\n",
    "# === Split data 80:20 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === TF-IDF + SVM ===\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# === Simpan model dan vectorizer ===\n",
    "joblib.dump(classifier, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "\n",
    "# === Ambil 10 query uji ===\n",
    "sample_df = df[df['ringkasan_fakta'].notnull() & (df['ringkasan_fakta'].str.strip() != '')]\n",
    "sample_df = sample_df.sample(n=10, random_state=42)\n",
    "\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"query\": row[\"ringkasan_fakta\"][:500],\n",
    "        \"ground_truth\": row[\"case_id\"]\n",
    "    }\n",
    "    for _, row in sample_df.iterrows()\n",
    "]\n",
    "\n",
    "with open(QUERY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUKSES] Model disimpan di :\", MODEL_PATH)\n",
    "print(\"[SUKSES] Vectorizer disimpan di :\", VECTORIZER_PATH)\n",
    "print(\"[SUKSES] 10 query uji disimpan di :\", QUERY_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d3817",
   "metadata": {},
   "source": [
    "### Tahap 4 – Solution Reuse\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menggunakan kembali solusi dari kasus-kasus terdahulu (putusan pengadilan) untuk memprediksi atau merekomendasikan solusi terhadap kasus baru yang serupa. Pendekatan ini merupakan inti dari prinsip Case-Based Reasoning (CBR) yang mengandalkan preseden sebagai referensi pemecahan masalah hukum.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "1. **Ekstraksi Solusi**\n",
    "   - Dari setiap kasus lama yang diretriev (hasil dari tahap 3), diambil bagian **amar putusan** atau bagian akhir dari dokumen sebagai `solusi_text`.\n",
    "   - Amar putusan disimpan dalam format `{case_id: solusi_text}`.\n",
    "\n",
    "2. **Algoritma Prediksi Solusi**\n",
    "   Dua pendekatan yang digunakan:\n",
    "   \n",
    "   - **CBR (TF-IDF + Cosine Similarity)**  \n",
    "     Sistem mengambil top-k kasus yang paling mirip berdasarkan cosine similarity. Amar putusan dari kasus top-1 dianggap sebagai solusi yang paling relevan untuk kasus baru.\n",
    "   \n",
    "   - **Supervised (TF-IDF + SVM)**  \n",
    "     Sistem memprediksi `case_id` menggunakan model klasifikasi SVM, kemudian mengambil amar putusan dari case tersebut sebagai solusi.\n",
    "\n",
    "3. **Ringkasan Solusi**\n",
    "   - Solusi yang ditampilkan disingkat agar lebih ringkas dan mudah dipahami, misalnya hanya mencakup 50 kata pertama dari teks amar putusan.\n",
    "\n",
    "4. **Demo Uji Coba**\n",
    "   - Sistem diuji menggunakan 10 query uji yang sudah disiapkan pada tahap sebelumnya.\n",
    "   - Fungsi `predict_outcome()` dijalankan untuk setiap query, dan hasil prediksi solusi dibandingkan dengan ground-truth atau dianalisis secara manual.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fungsi Utama\n",
    "\n",
    "- `retrieve(query: str, k: int = 5)`  \n",
    "  Mengambil top-k kasus terdekat berdasarkan cosine similarity atau top-1 dari hasil klasifikasi SVM.\n",
    "\n",
    "- `predict_outcome(query: str) -> Tuple[str, List[str]]`  \n",
    "  Mengembalikan ringkasan solusi dari kasus hasil prediksi dan daftar case ID referensi yang digunakan.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "Dua file CSV dihasilkan berisi hasil prediksi solusi:\n",
    "\n",
    "- `data/results/predictions_cosine.csv`  \n",
    "  Berisi prediksi solusi berdasarkan pendekatan CBR (TF-IDF + Cosine Similarity).\n",
    "\n",
    "- `data/results/predictions_svm.csv`  \n",
    "  Berisi prediksi solusi berdasarkan pendekatan supervised (TF-IDF + SVM).\n",
    "\n",
    "Setiap file memiliki kolom:\n",
    "- `query_id`: Nomor urut query.\n",
    "- `query`: Deskripsi singkat kasus baru.\n",
    "- `predicted_solution`: Ringkasan amar putusan yang diprediksi.\n",
    "- `top_5_case_ids`: Daftar case ID yang dijadikan referensi.\n",
    "\n",
    "---\n",
    "\n",
    "#### Catatan\n",
    "\n",
    "Pendekatan CBR memberikan fleksibilitas tinggi dalam menjawab kasus baru tanpa pelatihan ulang, tetapi bergantung pada kualitas kemiripan teks. Pendekatan supervised (SVM) lebih stabil pada data berlabel, tetapi membutuhkan pelatihan awal dan mungkin tidak generalis pada kasus baru yang sangat berbeda.\n",
    "\n",
    "Kedua pendekatan ini dapat dibandingkan dan dikombinasikan untuk mencapai hasil yang lebih optimal pada sistem prediksi putusan hukum berbasis CBR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "971eb6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Prediksi solusi berbasis cosine disimpan di: data/results/predictions_cosine.csv\n",
      "[SUKSES] Prediksi solusi berbasis SVM disimpan di   : data/results/predictions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Path konfigurasi ===\n",
    "CASE_DATA = \"data/processed/cases_extracted.csv\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "\n",
    "COSINE_PRED_PATH = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED_PATH = \"data/results/predictions_svm.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === Load semua aset ===\n",
    "df = pd.read_csv(CASE_DATA)\n",
    "with open(QUERY_PATH, encoding='utf-8') as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "classifier = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === Siapkan teks dan solusi dari kasus lama ===\n",
    "texts = df['ringkasan_fakta'].fillna(df['text_full']).astype(str).tolist()\n",
    "ids = df['case_id'].tolist()\n",
    "solutions = df['text_full'].astype(str).tolist()\n",
    "\n",
    "# === Vectorisasi semua kasus untuk cosine similarity ===\n",
    "tfidf_matrix = vectorizer.transform(texts)\n",
    "\n",
    "# === Fungsi Prediksi Solusi ===\n",
    "def predict_with_cosine(query: str, k: int = 5):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_k_idx = scores.argsort()[::-1][:k]\n",
    "    top_ids = [ids[i] for i in top_k_idx]\n",
    "    top_scores = [scores[i] for i in top_k_idx]\n",
    "    pred_text = solutions[top_k_idx[0]]\n",
    "    return pred_text[:300], top_ids, top_scores\n",
    "\n",
    "def predict_with_svm(query: str):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    pred_case_id = classifier.predict(query_vec)[0]\n",
    "    sol = df[df['case_id'] == pred_case_id]['text_full'].values[0]\n",
    "    return sol[:300], [pred_case_id]\n",
    "\n",
    "# === Jalankan prediksi untuk semua query ===\n",
    "cosine_results = []\n",
    "svm_results = []\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    query = q[\"query\"]\n",
    "\n",
    "    # COSINE\n",
    "    cosine_sol, cosine_top_ids, _ = predict_with_cosine(query, k=5)\n",
    "    cosine_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query,\n",
    "        \"predicted_solution\": cosine_sol,\n",
    "        \"top_5_case_ids\": cosine_top_ids\n",
    "    })\n",
    "\n",
    "    # SVM\n",
    "    svm_sol, svm_top_id = predict_with_svm(query)\n",
    "    svm_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query,\n",
    "        \"predicted_solution\": svm_sol,\n",
    "        \"top_1_case_id\": svm_top_id[0]\n",
    "    })\n",
    "\n",
    "# === Simpan hasil ===\n",
    "pd.DataFrame(cosine_results).to_csv(COSINE_PRED_PATH, index=False)\n",
    "pd.DataFrame(svm_results).to_csv(SVM_PRED_PATH, index=False)\n",
    "\n",
    "print(\"[SUKSES] Prediksi solusi berbasis cosine disimpan di:\", COSINE_PRED_PATH)\n",
    "print(\"[SUKSES] Prediksi solusi berbasis SVM disimpan di   :\", SVM_PRED_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ea4ec",
   "metadata": {},
   "source": [
    "### Tahap 5 – Model Evaluation\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Tahapan ini bertujuan untuk mengukur dan menganalisis performa sistem Case-Based Reasoning (CBR), baik dalam proses _retrieval_ (pencarian kasus serupa) maupun _prediction_ (penggunaan solusi dari kasus lama). Evaluasi dilakukan untuk menentukan pendekatan mana yang paling efektif: **TF-IDF + Cosine Similarity** atau **TF-IDF + SVM**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "##### 1. Evaluasi Retrieval\n",
    "\n",
    "- Evaluasi dilakukan terhadap hasil top-k retrieval dari pendekatan **TF-IDF + Cosine Similarity**.\n",
    "- Untuk setiap query, dicek apakah `ground_truth` termasuk dalam `top_k` hasil retrieval.\n",
    "- Metrik evaluasi yang digunakan:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "- Perhitungan dilakukan menggunakan fungsi dari `sklearn.metrics`.\n",
    "\n",
    "##### 2. Evaluasi Prediksi\n",
    "\n",
    "- Evaluasi dilakukan terhadap hasil prediksi top-1 dari pendekatan **TF-IDF + SVM**.\n",
    "- Hanya satu `case_id` diprediksi untuk setiap query, lalu dibandingkan dengan `ground_truth`.\n",
    "- Metrik yang digunakan sama dengan evaluasi retrieval:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "\n",
    "##### 3. Visualisasi & Laporan\n",
    "\n",
    "- Visualisasi hasil evaluasi dalam bentuk **grafik batang** (_bar chart_) yang membandingkan performa model.\n",
    "- Dilakukan **error analysis** terhadap query yang salah diprediksi, dan disimpan dalam file JSON.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementasi\n",
    "\n",
    "- Fungsi evaluasi:\n",
    "  - `eval_retrieval()`: untuk evaluasi top-k retrieval berdasarkan cosine similarity.\n",
    "  - `eval_prediction()`: untuk evaluasi top-1 prediksi menggunakan SVM.\n",
    "- Fungsi tambahan:\n",
    "  - `save_errors()`: menyimpan query yang gagal diprediksi dengan benar.\n",
    "- Evaluasi dilakukan berdasarkan file hasil prediksi:\n",
    "  - `data/results/predictions_cosine.csv`\n",
    "  - `data/results/predictions_svm.csv`\n",
    "- File ground truth:\n",
    "  - `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "- **Hasil Evaluasi**\n",
    "  - `data/eval/retrieval_metrics.csv`  \n",
    "    Berisi metrik evaluasi untuk TF-IDF + Cosine Similarity.\n",
    "  - `data/eval/prediction_metrics.csv`  \n",
    "    Berisi metrik evaluasi untuk TF-IDF + SVM.\n",
    "\n",
    "- **Visualisasi Performa**\n",
    "  - `data/eval/performance_comparison.png`  \n",
    "    Grafik perbandingan metrik antar model.\n",
    "\n",
    "- **Error Analysis**\n",
    "  - `data/eval/error_cases_cosine.json`  \n",
    "    Daftar query yang gagal ditemukan dengan benar oleh pendekatan cosine similarity.\n",
    "  - `data/eval/error_cases_svm.json`  \n",
    "    Daftar query yang gagal diprediksi oleh pendekatan SVM.\n",
    "\n",
    "---\n",
    "\n",
    "Tahap ini menjadi penentu efektivitas sistem CBR dalam konteks hukum perdata (PMH), dan menjadi dasar pemilihan model untuk deployment dan pengembangan lanjutan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd8e2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 5 - Evaluasi Model selesai\n",
      "- Hasil evaluasi retrieval disimpan di: data/eval/retrieval_metrics.csv\n",
      "- Hasil evaluasi prediksi disimpan di : data/eval/prediction_metrics.csv\n",
      "- Visualisasi disimpan di: data/eval/performance_comparison.png\n",
      "- Error cosine disimpan di: data/eval/error_cases_cosine.json\n",
      "- Error svm disimpan di: data/eval/error_cases_svm.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "\n",
    "# === PATH SETUP ===\n",
    "COSINE_PRED = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED = \"data/results/predictions_svm.csv\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "RETRIEVAL_METRIC_OUTPUT = \"data/eval/retrieval_metrics.csv\"\n",
    "PREDICTION_METRIC_OUTPUT = \"data/eval/prediction_metrics.csv\"\n",
    "ERROR_COSINE_OUTPUT = \"data/eval/error_cases_cosine.json\"\n",
    "ERROR_SVM_OUTPUT = \"data/eval/error_cases_svm.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD GROUND TRUTH ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "    gt_dict = {i+1: item[\"ground_truth\"] for i, item in enumerate(ground_truth_data)}\n",
    "\n",
    "# === EVALUASI RETRIEVAL COSINE (TOP-K) ===\n",
    "def eval_retrieval(pred_file: str, model_name: str, k: int = 5) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        try:\n",
    "            pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "        except:\n",
    "            pred_ids = []\n",
    "        pred_ids = pred_ids[:k]\n",
    "        gt = gt_dict[query_id]\n",
    "        hit = gt in pred_ids\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# === EVALUASI SVM PREDICTION (TOP-1) ===\n",
    "def eval_prediction(pred_file: str, model_name: str) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = gt_dict[query_id]\n",
    "        pred = row[\"top_1_case_id\"]\n",
    "        hit = pred == gt\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# === SIMPAN KASUS GAGAL (ERROR) ===\n",
    "def save_errors(pred_file: str, output_file: str, top_k: int = 5, mode: str = 'cosine'):\n",
    "    df = pd.read_csv(pred_file)\n",
    "    error_cases = []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = gt_dict[query_id]\n",
    "\n",
    "        if mode == 'cosine':\n",
    "            try:\n",
    "                pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "            except:\n",
    "                pred_ids = []\n",
    "            hit = gt in pred_ids[:top_k]\n",
    "        else:\n",
    "            pred_ids = [row[\"top_1_case_id\"]]\n",
    "            hit = gt == pred_ids[0]\n",
    "\n",
    "        if not hit:\n",
    "            error_cases.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"predicted\": pred_ids,\n",
    "                \"ground_truth\": gt\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === EKSEKUSI & SIMPAN HASIL ===\n",
    "retrieval_metrics = eval_retrieval(COSINE_PRED, \"TF-IDF + Cosine\")\n",
    "prediction_metrics = eval_prediction(SVM_PRED, \"TF-IDF + SVM\")\n",
    "\n",
    "pd.DataFrame([retrieval_metrics]).to_csv(RETRIEVAL_METRIC_OUTPUT, index=False)\n",
    "pd.DataFrame([prediction_metrics]).to_csv(PREDICTION_METRIC_OUTPUT, index=False)\n",
    "\n",
    "save_errors(COSINE_PRED, ERROR_COSINE_OUTPUT, mode='cosine')\n",
    "save_errors(SVM_PRED, ERROR_SVM_OUTPUT, mode='svm')\n",
    "\n",
    "# === VISUALISASI ===\n",
    "combined_df = pd.DataFrame([retrieval_metrics, prediction_metrics])\n",
    "combined_df.set_index(\"model\")[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(\n",
    "    kind=\"bar\", figsize=(10, 6), title=\"Perbandingan Model\"\n",
    ")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/eval/performance_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"[SUKSES] Tahap 5 - Evaluasi Model selesai\")\n",
    "print(f\"- Hasil evaluasi retrieval disimpan di: {RETRIEVAL_METRIC_OUTPUT}\")\n",
    "print(f\"- Hasil evaluasi prediksi disimpan di : {PREDICTION_METRIC_OUTPUT}\")\n",
    "print(\"- Visualisasi disimpan di: data/eval/performance_comparison.png\")\n",
    "print(f\"- Error cosine disimpan di: {ERROR_COSINE_OUTPUT}\")\n",
    "print(f\"- Error svm disimpan di: {ERROR_SVM_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170d3a7",
   "metadata": {},
   "source": [
    "### Kesimpulan\n",
    "\n",
    "---\n",
    "\n",
    "### Penutup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
