{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8f9f0e",
   "metadata": {},
   "source": [
    "---\n",
    "# Perbandingan Pendekatan Case-Based Reasoning Berbasis TF-IDF: Cosine Similarity vs SVM dalam Analisis Putusan Perdata PMH\n",
    "\n",
    "Anggota Kelompok :\n",
    "\n",
    "1. Rofiq Samanhudi - 202210370311260\n",
    "\n",
    "2. Muhammad Ikbar Ananda Sulistio - 202210370311236\n",
    "\n",
    "## Tugas Besar Mata Kuliah Penalaran Komputer (B)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab23841",
   "metadata": {},
   "source": [
    "## Tahap 1 – Membangun Case Base\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk mengumpulkan dan mempersiapkan korpus dokumen putusan pengadilan dalam bentuk teks yang bersih dan terstruktur. Dokumen ini akan menjadi basis bagi seluruh proses reasoning pada sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Seleksi dan Pengunduhan Dokumen\n",
    "\n",
    "- Pemilihan jenis perkara: **Perdata - Perbuatan Melawan Hukum (PMH)**.\n",
    "- Proses scraping dilakukan dari situs putusan Mahkamah Agung.\n",
    "- File unduhan disimpan dalam bentuk PDF di direktori: `PMH_PDF/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Konversi dan Ekstraksi Teks\n",
    "\n",
    "- File PDF dikonversi menjadi teks menggunakan library `pdfminer`.\n",
    "- Setiap file diproses untuk mengambil seluruh isi putusan sebagai teks mentah.\n",
    "- Output teks disimpan per dokumen di folder: `data/raw/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Pembersihan Teks (Cleaning)\n",
    "\n",
    "- Menghapus watermark, header, footer, dan disclaimer resmi dari Mahkamah Agung.\n",
    "- Normalisasi teks dilakukan dengan:\n",
    "  - Menghilangkan baris kosong ganda.\n",
    "  - Mengubah huruf menjadi lowercase.\n",
    "  - Menghapus spasi berlebih.\n",
    "- Rasio retensi konten dihitung untuk mengevaluasi hasil pembersihan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Validasi dan Logging\n",
    "\n",
    "- Setiap file yang berhasil diproses dilaporkan ke dalam log:\n",
    "  - Nomor dokumen\n",
    "  - Status pembersihan (OK atau rendah)\n",
    "  - Lokasi penyimpanan\n",
    "  - Tanggal dan waktu proses\n",
    "- Log disimpan di file: `logs/cleaning.log`.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Tahap Ini\n",
    "\n",
    "- Total dokumen yang diproses: **70 kasus**.\n",
    "- Dokumen teks bersih disimpan di: `data/raw/`\n",
    "- Log proses dan status pembersihan: `logs/cleaning.log`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap pertama ini berhasil membentuk korpus awal untuk sistem CBR. Dokumen telah dikonversi ke format teks (>80%) dan dibersihkan secara sistematis, siap untuk tahap representasi selanjutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7913038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning PDF text extraction...\n",
      "[OK] case_001 processed (96.35% valid).\n",
      "[OK] case_002 processed (96.25% valid).\n",
      "[OK] case_003 processed (96.36% valid).\n",
      "[OK] case_004 processed (96.48% valid).\n",
      "[OK] case_005 processed (96.34% valid).\n",
      "[OK] case_006 processed (96.38% valid).\n",
      "[OK] case_007 processed (96.10% valid).\n",
      "[OK] case_008 processed (96.30% valid).\n",
      "[OK] case_009 processed (96.43% valid).\n",
      "[OK] case_010 processed (96.40% valid).\n",
      "[OK] case_011 processed (96.42% valid).\n",
      "[OK] case_012 processed (96.45% valid).\n",
      "[OK] case_013 processed (95.81% valid).\n",
      "[OK] case_014 processed (96.56% valid).\n",
      "[OK] case_015 processed (96.24% valid).\n",
      "[OK] case_016 processed (96.26% valid).\n",
      "[OK] case_017 processed (96.22% valid).\n",
      "[OK] case_018 processed (96.21% valid).\n",
      "[OK] case_019 processed (96.46% valid).\n",
      "[OK] case_020 processed (96.71% valid).\n",
      "[OK] case_021 processed (96.55% valid).\n",
      "[OK] case_022 processed (96.23% valid).\n",
      "[OK] case_023 processed (96.41% valid).\n",
      "[OK] case_024 processed (96.18% valid).\n",
      "[OK] case_025 processed (96.45% valid).\n",
      "[OK] case_026 processed (96.25% valid).\n",
      "[OK] case_027 processed (96.33% valid).\n",
      "[OK] case_028 processed (96.26% valid).\n",
      "[OK] case_029 processed (96.01% valid).\n",
      "[OK] case_030 processed (96.23% valid).\n",
      "[OK] case_031 processed (96.22% valid).\n",
      "[OK] case_032 processed (96.13% valid).\n",
      "[OK] case_033 processed (96.38% valid).\n",
      "[OK] case_034 processed (96.19% valid).\n",
      "[OK] case_035 processed (96.17% valid).\n",
      "[OK] case_036 processed (96.12% valid).\n",
      "[OK] case_037 processed (96.19% valid).\n",
      "[OK] case_038 processed (96.31% valid).\n",
      "[OK] case_039 processed (96.24% valid).\n",
      "[OK] case_040 processed (96.24% valid).\n",
      "[OK] case_041 processed (96.13% valid).\n",
      "[OK] case_042 processed (96.49% valid).\n",
      "[OK] case_043 processed (96.43% valid).\n",
      "[OK] case_044 processed (96.25% valid).\n",
      "[OK] case_045 processed (96.35% valid).\n",
      "[OK] case_046 processed (96.42% valid).\n",
      "[OK] case_047 processed (96.32% valid).\n",
      "[OK] case_048 processed (96.58% valid).\n",
      "[OK] case_049 processed (96.35% valid).\n",
      "[OK] case_050 processed (96.28% valid).\n",
      "[OK] case_051 processed (96.55% valid).\n",
      "[OK] case_052 processed (96.33% valid).\n",
      "[OK] case_053 processed (96.34% valid).\n",
      "[OK] case_054 processed (96.52% valid).\n",
      "[OK] case_055 processed (96.47% valid).\n",
      "[OK] case_056 processed (96.50% valid).\n",
      "[OK] case_057 processed (96.34% valid).\n",
      "[OK] case_058 processed (96.49% valid).\n",
      "[OK] case_059 processed (96.12% valid).\n",
      "[OK] case_060 processed (96.22% valid).\n",
      "[OK] case_061 processed (96.33% valid).\n",
      "[OK] case_062 processed (96.62% valid).\n",
      "[OK] case_063 processed (96.37% valid).\n",
      "[OK] case_064 processed (96.24% valid).\n",
      "[OK] case_065 processed (96.47% valid).\n",
      "[OK] case_066 processed (96.21% valid).\n",
      "[OK] case_067 processed (96.41% valid).\n",
      "[OK] case_068 processed (96.38% valid).\n",
      "[OK] case_069 processed (96.50% valid).\n",
      "[OK] case_070 processed (96.57% valid).\n",
      "PDF processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from pdfminer.high_level import extract_text\n",
    "from datetime import datetime\n",
    "\n",
    "# === Directory configuration ===\n",
    "SOURCE_DIR = 'PMH_PDF'\n",
    "DEST_DIR = 'data/raw'\n",
    "LOG_DEST = 'logs/cleaning.log'\n",
    "\n",
    "def purify_text(input_text):\n",
    "    \"\"\"Remove unwanted headers, footers, and disclaimers from extracted text.\"\"\"\n",
    "    initial_length = len(input_text) if input_text else 1  # Prevent division by zero\n",
    "\n",
    "    # Eliminate watermarks and page indicators\n",
    "    processed_text = input_text\n",
    "    processed_text = processed_text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    processed_text = processed_text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    processed_text = re.sub(r'halaman\\s*\\d+', '', processed_text, flags=re.IGNORECASE)\n",
    "    processed_text = processed_text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    \n",
    "    # Record length after initial cleaning\n",
    "    intermediate_length = len(processed_text)\n",
    "    \n",
    "    # Remove disclaimer content\n",
    "    processed_text = processed_text.replace(\"Disclaimer\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\")\n",
    "\n",
    "    # Final text normalization\n",
    "    processed_text = processed_text.lower()\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "\n",
    "    # Calculate content retention ratio\n",
    "    retention_ratio = intermediate_length / initial_length\n",
    "\n",
    "    return processed_text, retention_ratio\n",
    "\n",
    "def record_log(case_id, retention_ratio):\n",
    "    \"\"\"Log processing details to a file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(LOG_DEST), exist_ok=True)\n",
    "    with open(LOG_DEST, 'a', encoding='utf-8') as log:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        log.write(f\"[{timestamp}] {case_id} | Content Integrity: {retention_ratio:.2%}\\n\")\n",
    "\n",
    "def handle_pdf_batch():\n",
    "    \"\"\"Process all PDFs in the source directory.\"\"\"\n",
    "    os.makedirs(DEST_DIR, exist_ok=True)\n",
    "    pdf_paths = sorted(glob.glob(os.path.join(SOURCE_DIR, '*.pdf')))\n",
    "    \n",
    "    if not pdf_paths:\n",
    "        print(\"No PDFs found in the source directory.\")\n",
    "        return\n",
    "\n",
    "    for index, pdf in enumerate(pdf_paths, 1):\n",
    "        case_id = f\"case_{index:03d}\"\n",
    "        output_path = os.path.join(DEST_DIR, f\"{case_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            raw_text = extract_text(pdf)\n",
    "\n",
    "            # Clean and validate text\n",
    "            final_text, integrity_ratio = purify_text(raw_text)\n",
    "\n",
    "            # Check content integrity\n",
    "            if integrity_ratio < 0.8:\n",
    "                print(f\"[WARNING] {case_id} retains only {integrity_ratio:.2%} of content.\")\n",
    "            else:\n",
    "                print(f\"[OK] {case_id} processed ({integrity_ratio:.2%} valid).\")\n",
    "                # Save cleaned text if valid\n",
    "                with open(output_path, 'w', encoding='utf-8') as output:\n",
    "                    output.write(final_text)\n",
    "\n",
    "            # Log the result\n",
    "            record_log(case_id, integrity_ratio)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"[ERROR] Failed to process {pdf}: {str(err)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Initiate PDF processing workflow.\"\"\"\n",
    "    print(\"Beginning PDF text extraction...\")\n",
    "    handle_pdf_batch()\n",
    "    print(\"PDF processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645feb1c",
   "metadata": {},
   "source": [
    "## Tahap 2 – Case Representation\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk merepresentasikan setiap putusan dalam struktur data yang terorganisir. Representasi ini akan menjadi basis data terstruktur yang siap digunakan untuk proses retrieval dan analisis pada sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Ekstraksi Metadata\n",
    "\n",
    "Setiap dokumen hasil pembersihan teks dianalisis untuk mengekstrak informasi penting sebagai metadata menggunakan pendekatan berbasis pola (regex) dan pemrosesan teks. Metadata yang diekstrak meliputi:\n",
    "\n",
    "- Nomor Perkara (`no_perkara`)\n",
    "- Tanggal Putusan (`tanggal`)\n",
    "- Ringkasan Fakta (`ringkasan_fakta`)\n",
    "- Pasal yang digunakan atau dasar hukum (`pasal`)\n",
    "- Pihak-pihak yang terlibat (tergugat dan penggugat)\n",
    "- Isi teks lengkap dari putusan (`text_full`)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Penyimpanan Data Terstruktur\n",
    "\n",
    "Hasil ekstraksi metadata disimpan dalam dua format file untuk kemudahan pemrosesan:\n",
    "\n",
    "- **CSV** → `data/processed/cases_extracted.csv`\n",
    "- **JSON** → `data/processed/cases_extracted.json`\n",
    "\n",
    "Struktur data untuk masing-masing kasus mencakup kolom:\n",
    "- `case_id`\n",
    "- `no_perkara`\n",
    "- `tanggal`\n",
    "- `ringkasan_fakta`\n",
    "- `pasal`\n",
    "- `pihak`\n",
    "- `text_full`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Feature Engineering\n",
    "\n",
    "Untuk meningkatkan kualitas representasi kasus, dilakukan proses rekayasa fitur (feature engineering) sebagai berikut:\n",
    "\n",
    "- **Panjang Teks (Length)**: Menghitung jumlah kata dalam `ringkasan_fakta`.\n",
    "- **Bag-of-Words (BoW)**: Menghitung frekuensi kata yang muncul dalam ringkasan.\n",
    "- **QA-Pairs Sederhana**: Menghasilkan pasangan pertanyaan dan jawaban berbasis konten ringkasan, misalnya:\n",
    "  - Apa nomor perkaranya?\n",
    "  - Apa pasal yang relevan?\n",
    "  - Siapa pihak tergugat?\n",
    "  - Apa ringkasan faktanya?\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Penyimpanan Fitur\n",
    "\n",
    "Fitur-fitur yang telah direkayasa disimpan dalam file JSON untuk fleksibilitas dalam analisis selanjutnya:\n",
    "\n",
    "- `data/processed/features_length.json`\n",
    "- `data/processed/features_bow.json`\n",
    "- `data/processed/features_qa_pairs.json`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap representasi berhasil membentuk struktur data yang terorganisir untuk semua kasus yang tersedia. Masing-masing kasus telah dilengkapi dengan metadata penting dan fitur tambahan untuk mendukung proses retrieval dan reasoning pada tahapan berikutnya dalam sistem CBR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4676e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] 70 kasus berhasil diproses dan disimpan:\n",
      "CSV → data/processed\\cases_extracted.csv\n",
      "JSON → data/processed\\cases_extracted.json\n",
      "Length → data/processed\\features_length.json\n",
      "BoW → data/processed\\features_bow.json\n",
      "QA Pairs → data/processed\\features_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "RAW_FOLDER = 'data/raw'\n",
    "PROCESSED_FOLDER = 'data/processed'\n",
    "\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_metadata(text):\n",
    "    metadata = {}\n",
    "\n",
    "    # Nomor Perkara\n",
    "    match = re.search(r'Nomor\\s*:\\s*(\\S+)', text, re.IGNORECASE)\n",
    "    metadata['no_perkara'] = match.group(1) if match else ''\n",
    "\n",
    "    # Tanggal Putusan\n",
    "    match = re.search(r'\\b(?:putusan|diputuskan)\\s*(?:pada|tanggal)?\\s*(\\d{1,2}\\s+\\w+\\s+\\d{4})', text, re.IGNORECASE)\n",
    "    metadata['tanggal'] = match.group(1) if match else ''\n",
    "\n",
    "    # Pasal yang disebutkan\n",
    "    match = re.findall(r'Pasal\\s+\\d+\\s+[^\\n.,;]*', text, re.IGNORECASE)\n",
    "    metadata['pasal'] = match if match else []\n",
    "\n",
    "    # Ringkasan Fakta Sederhana\n",
    "    match = re.search(r'(menimbang\\s+bahwa.*?)((?:menimbang|mengingat|memperhatikan)\\b.*)', text, re.IGNORECASE | re.DOTALL)\n",
    "    metadata['ringkasan_fakta'] = match.group(1).strip() if match else ''\n",
    "\n",
    "    # Terdakwa & Korban (opsional)\n",
    "    terdakwa = re.findall(r'terdakwa(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    korban = re.findall(r'korban(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    metadata['pihak'] = {\n",
    "        'terdakwa': terdakwa[0] if terdakwa else '',\n",
    "        'korban': korban[0] if korban else ''\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def feature_engineering(text, metadata):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    length = len(tokens)\n",
    "    bow = dict(Counter(tokens))\n",
    "\n",
    "    qa_pairs = {\n",
    "        \"Apa nomor perkaranya?\": metadata.get(\"no_perkara\", \"\"),\n",
    "        \"Apa pasal yang dilanggar?\": \", \".join(metadata.get(\"pasal\", [])),\n",
    "        \"Siapa terdakwanya?\": metadata.get(\"pihak\", {}).get(\"terdakwa\", \"\"),\n",
    "        \"Siapa korbannya?\": metadata.get(\"pihak\", {}).get(\"korban\", \"\")\n",
    "    }\n",
    "\n",
    "    return length, bow, qa_pairs\n",
    "\n",
    "def process_all():\n",
    "    # === Cek folder dan file .txt ===\n",
    "    if not os.path.exists(RAW_FOLDER):\n",
    "        raise FileNotFoundError(f\"[ERROR] Folder '{RAW_FOLDER}' tidak ditemukan. Harap buat dan isi dengan file .txt.\")\n",
    "\n",
    "    txt_files = sorted([f for f in os.listdir(RAW_FOLDER) if f.endswith('.txt')])\n",
    "    if not txt_files:\n",
    "        raise FileNotFoundError(f\"[ERROR] Tidak ada file .txt dalam '{RAW_FOLDER}'. Harap isi folder dengan putusan .txt.\")\n",
    "\n",
    "    all_cases = []\n",
    "    features_length = {}\n",
    "    features_bow = {}\n",
    "    features_qa = {}\n",
    "\n",
    "    for i, fname in enumerate(txt_files, 1):\n",
    "        path = os.path.join(RAW_FOLDER, fname)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "\n",
    "        if not text:\n",
    "            print(f\"[PERINGATAN] File kosong dilewati: {fname}\")\n",
    "            continue\n",
    "\n",
    "        case_id = f\"case_{i:03d}\"\n",
    "        metadata = extract_metadata(text)\n",
    "        metadata['case_id'] = case_id\n",
    "        metadata['text_full'] = text\n",
    "\n",
    "        length, bow, qa = feature_engineering(text, metadata)\n",
    "\n",
    "        all_cases.append(metadata)\n",
    "        features_length[case_id] = length\n",
    "        features_bow[case_id] = bow\n",
    "        features_qa[case_id] = qa\n",
    "\n",
    "    # Simpan CSV\n",
    "    csv_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.csv')\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['case_id', 'no_perkara', 'tanggal', 'ringkasan_fakta', 'pasal', 'pihak', 'text_full']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_cases:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Simpan JSON\n",
    "    json_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Simpan fitur\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_length.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_length, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_bow.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_bow, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_qa, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[SUKSES] {len(all_cases)} kasus berhasil diproses dan disimpan:\")\n",
    "    print(\"CSV →\", csv_path)\n",
    "    print(\"JSON →\", json_path)\n",
    "    print(\"Length →\", os.path.join(PROCESSED_FOLDER, 'features_length.json'))\n",
    "    print(\"BoW →\", os.path.join(PROCESSED_FOLDER, 'features_bow.json'))\n",
    "    print(\"QA Pairs →\", os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ef46e",
   "metadata": {},
   "source": [
    "## Tahap 3 – Case Retrieval\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menemukan kasus-kasus lama yang paling relevan dan mirip dengan kasus baru (query) yang diajukan. Proses ini merupakan bagian inti dari sistem Case-Based Reasoning (CBR), yang mendukung pencarian preseden hukum berdasarkan kemiripan konten.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Representasi Vektor\n",
    "\n",
    "Setiap ringkasan fakta dari putusan diubah menjadi representasi vektor menggunakan algoritma **TF-IDF (Term Frequency – Inverse Document Frequency)**. Representasi ini mengubah teks menjadi format numerik agar bisa digunakan untuk penghitungan kemiripan dan klasifikasi.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Splitting Data\n",
    "\n",
    "Dataset dibagi menjadi dua bagian:\n",
    "- **Data latih (training set)** sebanyak 80%\n",
    "- **Data uji (test set)** sebanyak 20%\n",
    "\n",
    "Splitting ini digunakan untuk pelatihan model klasifikasi berbasis TF-IDF + SVM, serta evaluasi awal performa retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Model Retrieval\n",
    "\n",
    "Dua pendekatan digunakan untuk proses retrieval:\n",
    "\n",
    "- **a. TF-IDF + Cosine Similarity (CBR Approach)**  \n",
    "  Menggunakan cosine similarity untuk mengukur kemiripan vektor TF-IDF antara query dan semua kasus dalam basis data. Top-k kasus dengan skor tertinggi dianggap sebagai yang paling relevan.\n",
    "\n",
    "- **b. TF-IDF + Support Vector Machine (SVM)**  \n",
    "  Menggunakan pendekatan supervised learning. Model SVM dilatih untuk memetakan ringkasan fakta ke `case_id`. Diberikan sebuah query, model akan memprediksi case_id yang paling sesuai.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Fungsi Retrieval\n",
    "\n",
    "Fungsi retrieval disiapkan untuk masing-masing pendekatan:\n",
    "\n",
    "- `retrieve_cosine(query: str, k: int = 5)`  \n",
    "  Mengembalikan top-k `case_id` dengan skor cosine tertinggi dari seluruh basis data.\n",
    "\n",
    "- `retrieve_svm(query: str)`  \n",
    "  Mengembalikan satu `case_id` yang diprediksi oleh model SVM berdasarkan klasifikasi.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Pengujian Awal\n",
    "\n",
    "Sebanyak 10 query uji disiapkan beserta ground-truth case ID. Untuk masing-masing query:\n",
    "- Dihitung top-5 hasil cosine similarity.\n",
    "- Diprediksi top-1 hasil dari model SVM.\n",
    "- Hasilnya dibandingkan dengan ground-truth.\n",
    "\n",
    "Query uji disimpan di:\n",
    "- `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "File hasil yang dihasilkan dari tahap ini meliputi:\n",
    "- Model klasifikasi (SVM): `03_retrieval_model.pkl`\n",
    "- TF-IDF Vectorizer: `03_vectorizer.pkl`\n",
    "- Dataset query uji: `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "Tahap Case Retrieval telah berhasil diimplementasikan menggunakan dua pendekatan: unsupervised berbasis kemiripan teks dan supervised berbasis klasifikasi. Keduanya siap digunakan untuk evaluasi dan prediksi solusi dalam tahapan berikutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e920067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Model disimpan di : 03_retrieval_model.pkl\n",
      "[SUKSES] Vectorizer disimpan di : 03_vectorizer.pkl\n",
      "[SUKSES] 10 query uji disimpan di : data/eval/queries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# === Konfigurasi path ===\n",
    "CSV_PATH = \"data/processed/cases_extracted.csv\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === Muat data CSV ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "texts = df['ringkasan_fakta'].fillna(df['text_full']).astype(str)\n",
    "labels = df['case_id'].astype(str)\n",
    "\n",
    "# === Split data 80:20 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === TF-IDF + SVM ===\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# === Simpan model dan vectorizer ===\n",
    "joblib.dump(classifier, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "\n",
    "# === Ambil 10 query uji ===\n",
    "sample_df = df[df['ringkasan_fakta'].notnull() & (df['ringkasan_fakta'].str.strip() != '')]\n",
    "sample_df = sample_df.sample(n=10, random_state=42)\n",
    "\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"query\": row[\"ringkasan_fakta\"][:500],\n",
    "        \"ground_truth\": row[\"case_id\"]\n",
    "    }\n",
    "    for _, row in sample_df.iterrows()\n",
    "]\n",
    "\n",
    "with open(QUERY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUKSES] Model disimpan di :\", MODEL_PATH)\n",
    "print(\"[SUKSES] Vectorizer disimpan di :\", VECTORIZER_PATH)\n",
    "print(\"[SUKSES] 10 query uji disimpan di :\", QUERY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d3817",
   "metadata": {},
   "source": [
    "### Tahap 4 – Solution Reuse\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menggunakan kembali solusi dari kasus-kasus terdahulu (putusan pengadilan) untuk memprediksi atau merekomendasikan solusi terhadap kasus baru yang serupa. Pendekatan ini merupakan inti dari prinsip *Case-Based Reasoning (CBR)* yang mengandalkan preseden sebagai referensi pemecahan masalah hukum.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "1. **Ekstraksi Solusi**\n",
    "   - Dari setiap kasus lama yang diretriev (hasil dari tahap 3), diambil bagian **amar putusan** atau bagian akhir dari dokumen sebagai `solusi_text`.\n",
    "   - Amar putusan disimpan dalam format `{case_id: solusi_text}`.\n",
    "\n",
    "2. **Algoritma Prediksi Solusi**\n",
    "\n",
    "   Terdapat dua pendekatan utama:\n",
    "\n",
    "   - **CBR (TF-IDF + Cosine Similarity)**  \n",
    "     Sistem mengambil *top-k* kasus paling mirip berdasarkan cosine similarity, lalu menerapkan salah satu dari 3 strategi:\n",
    "\n",
    "     - `weighted`: memilih solusi dengan total skor similarity tertinggi.\n",
    "     - `majority`: memilih solusi yang paling sering muncul di antara *top-k*.\n",
    "     - `hybrid`: kombinasi antara skor similarity dan frekuensi solusi.\n",
    "\n",
    "   - **Supervised (TF-IDF + SVM)**  \n",
    "     Sistem memprediksi `case_id` menggunakan klasifikasi SVM, lalu mengambil solusi dari kasus tersebut.\n",
    "\n",
    "3. **Ringkasan Solusi**\n",
    "   - Solusi yang dipilih dipotong sepanjang 300 karakter dari awal teks untuk menghasilkan ringkasan yang singkat dan informatif.\n",
    "\n",
    "4. **Demo Uji Coba**\n",
    "   - Sistem diuji menggunakan 10 query uji dari `queries.json` yang telah disiapkan pada tahap 3.\n",
    "   - Fungsi `predict_with_cosine()` dan `predict_with_svm()` dijalankan untuk setiap query.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fungsi Utama\n",
    "\n",
    "- `predict_with_cosine(query: str, k=5, mode=\"weighted\")`  \n",
    "  Melakukan pencarian top-k kasus dengan cosine similarity dan menghasilkan solusi berdasarkan:\n",
    "  - `\"weighted\"`: solusi dengan total skor similarity tertinggi.\n",
    "  - `\"majority\"`: solusi terbanyak di antara hasil retrieval.\n",
    "  - `\"hybrid\"`: kombinasi bobot similarity dan jumlah kemunculan.\n",
    "\n",
    "- `predict_with_svm(query: str)`  \n",
    "  Menggunakan model klasifikasi SVM untuk memprediksi `case_id`, lalu mengambil solusi dari dokumen tersebut.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "Dua file CSV utama dihasilkan:\n",
    "\n",
    "- `data/results/predictions_cosine.csv`  \n",
    "  Berisi hasil prediksi solusi berdasarkan pendekatan CBR Cosine Similarity (mode: `weighted` secara default).\n",
    "\n",
    "- `data/results/predictions_svm.csv`  \n",
    "  Berisi hasil prediksi solusi berdasarkan klasifikasi SVM.\n",
    "\n",
    "Kolom-kolom pada file CSV:\n",
    "\n",
    "- `query_id`: Nomor urut query.\n",
    "- `query`: Teks ringkasan kasus baru.\n",
    "- `predicted_solution`: Solusi yang diprediksi dari dokumen mirip.\n",
    "- `top_5_case_ids`: Daftar case ID referensi (Cosine).\n",
    "- `top_1_case_id`: Case ID prediksi tunggal (SVM).\n",
    "\n",
    "---\n",
    "\n",
    "#### Catatan\n",
    "\n",
    "- Pendekatan **CBR (Cosine)** bersifat fleksibel, tidak membutuhkan pelatihan ulang, dan cocok digunakan untuk kasus baru yang belum pernah terlihat.\n",
    "- Pendekatan **SVM** cocok untuk klasifikasi cepat pada data berlabel, tetapi perlu pelatihan awal dan lebih kaku terhadap variasi baru.\n",
    "- Mode `hybrid` dapat menjadi alternatif untuk menggabungkan akurasi skor dan kestabilan voting.\n",
    "- Mode cosine dapat disesuaikan dengan parameter `mode` saat pemanggilan `predict_with_cosine()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971eb6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Hasil prediksi COSINE disimpan di: data/results/predictions_cosine.csv\n",
      "[SUKSES] Hasil prediksi SVM disimpan di   : data/results/predictions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === PATH KONFIGURASI ===\n",
    "CASE_DATA = \"data/processed/cases_extracted.csv\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "COSINE_OUTPUT = \"data/results/predictions_cosine.csv\"\n",
    "SVM_OUTPUT = \"data/results/predictions_svm.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === Load data dan model ===\n",
    "df = pd.read_csv(CASE_DATA)\n",
    "with open(QUERY_PATH, encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "classifier = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === Siapkan korpus dokumen dan solusi ===\n",
    "texts = df[\"ringkasan_fakta\"].fillna(df[\"text_full\"]).astype(str).tolist()\n",
    "case_ids = df[\"case_id\"].astype(str).tolist()\n",
    "solutions = df[\"text_full\"].astype(str).tolist()\n",
    "\n",
    "tfidf_matrix = vectorizer.transform(texts)\n",
    "case_solutions = dict(zip(case_ids, solutions))\n",
    "case_id_to_text = dict(zip(case_ids, texts))\n",
    "\n",
    "# === COSINE SIMILARITY DENGAN MODE: majority / weighted / hybrid ===\n",
    "def predict_with_cosine(query: str, k: int = 5, mode: str = \"weighted\"):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_k_idx = scores.argsort()[::-1][:k]\n",
    "    \n",
    "    solution_scores = defaultdict(float)\n",
    "    solution_counts = Counter()\n",
    "    \n",
    "    for i in top_k_idx:\n",
    "        cid = case_ids[i]\n",
    "        score = float(scores[i])\n",
    "        solution = case_solutions.get(cid, \"\")[:300]\n",
    "\n",
    "        if mode == \"weighted\":\n",
    "            solution_scores[solution] += score\n",
    "        elif mode == \"majority\":\n",
    "            solution_counts[solution] += 1\n",
    "        elif mode == \"hybrid\":\n",
    "            solution_scores[solution] += score * 0.7 + solution_counts[solution] * 0.3\n",
    "        else:\n",
    "            raise ValueError(f\"Mode tidak dikenali: {mode}\")\n",
    "\n",
    "    # Pilih solusi terbaik\n",
    "    if mode in (\"weighted\", \"hybrid\"):\n",
    "        predicted_solution = max(solution_scores.items(), key=lambda x: x[1])[0]\n",
    "    else:  # majority\n",
    "        predicted_solution = solution_counts.most_common(1)[0][0]\n",
    "\n",
    "    top_ids = [case_ids[i] for i in top_k_idx]\n",
    "    return predicted_solution, top_ids\n",
    "\n",
    "# === SVM CLASSIFICATION ===\n",
    "def predict_with_svm(query: str):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    pred_case_id = classifier.predict(query_vec)[0]\n",
    "\n",
    "    sol = case_solutions.get(str(pred_case_id), \"\")\n",
    "    return sol[:300], [pred_case_id]\n",
    "\n",
    "# === Jalankan untuk semua query ===\n",
    "cosine_results = []\n",
    "svm_results = []\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    query_text = q[\"query\"]\n",
    "\n",
    "    # COSINE - mode: \"weighted\" | \"majority\" | \"hybrid\"\n",
    "    cosine_pred, cosine_ids = predict_with_cosine(query_text, mode=\"weighted\")\n",
    "    cosine_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query_text,\n",
    "        \"predicted_solution\": cosine_pred,\n",
    "        \"top_5_case_ids\": cosine_ids\n",
    "    })\n",
    "\n",
    "    # SVM\n",
    "    svm_pred, svm_id = predict_with_svm(query_text)\n",
    "    svm_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query_text,\n",
    "        \"predicted_solution\": svm_pred,\n",
    "        \"top_1_case_id\": svm_id[0]\n",
    "    })\n",
    "\n",
    "# === Simpan hasil ke CSV ===\n",
    "pd.DataFrame(cosine_results).to_csv(COSINE_OUTPUT, index=False)\n",
    "pd.DataFrame(svm_results).to_csv(SVM_OUTPUT, index=False)\n",
    "\n",
    "print(\"[SUKSES] Hasil prediksi COSINE disimpan di:\", COSINE_OUTPUT)\n",
    "print(\"[SUKSES] Hasil prediksi SVM disimpan di   :\", SVM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ea4ec",
   "metadata": {},
   "source": [
    "### Tahap 5 – Model Evaluation\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Tahapan ini bertujuan untuk mengukur dan menganalisis performa sistem Case-Based Reasoning (CBR), baik dalam proses _retrieval_ (pencarian kasus serupa) maupun _prediction_ (penggunaan solusi dari kasus lama). Evaluasi dilakukan untuk menentukan pendekatan mana yang paling efektif: **TF-IDF + Cosine Similarity** atau **TF-IDF + SVM**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "##### 1. Evaluasi Retrieval\n",
    "\n",
    "- Evaluasi dilakukan terhadap hasil top-k retrieval dari pendekatan **TF-IDF + Cosine Similarity**.\n",
    "- Untuk setiap query, dicek apakah `ground_truth` termasuk dalam `top_k` hasil retrieval.\n",
    "- Metrik evaluasi yang digunakan:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "- Perhitungan dilakukan menggunakan fungsi dari `sklearn.metrics`.\n",
    "\n",
    "##### 2. Evaluasi Prediksi\n",
    "\n",
    "- Evaluasi dilakukan terhadap hasil prediksi top-1 dari pendekatan **TF-IDF + SVM**.\n",
    "- Hanya satu `case_id` diprediksi untuk setiap query, lalu dibandingkan dengan `ground_truth`.\n",
    "- Metrik yang digunakan sama dengan evaluasi retrieval:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "\n",
    "##### 3. Visualisasi & Laporan\n",
    "\n",
    "- Visualisasi hasil evaluasi dalam bentuk **grafik batang** (_bar chart_) yang membandingkan performa model.\n",
    "- Dilakukan **error analysis** terhadap query yang salah diprediksi, dan disimpan dalam file JSON.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementasi\n",
    "\n",
    "- Fungsi evaluasi:\n",
    "  - `eval_retrieval()`: untuk evaluasi top-k retrieval berdasarkan cosine similarity.\n",
    "  - `eval_prediction()`: untuk evaluasi top-1 prediksi menggunakan SVM.\n",
    "- Fungsi tambahan:\n",
    "  - `save_errors()`: menyimpan query yang gagal diprediksi dengan benar.\n",
    "- Evaluasi dilakukan berdasarkan file hasil prediksi:\n",
    "  - `data/results/predictions_cosine.csv`\n",
    "  - `data/results/predictions_svm.csv`\n",
    "- File ground truth:\n",
    "  - `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "- **Hasil Evaluasi**\n",
    "  - `data/eval/retrieval_metrics.csv`  \n",
    "    Berisi metrik evaluasi untuk TF-IDF + Cosine Similarity.\n",
    "  - `data/eval/prediction_metrics.csv`  \n",
    "    Berisi metrik evaluasi untuk TF-IDF + SVM.\n",
    "\n",
    "- **Visualisasi Performa**\n",
    "  - `data/eval/performance_comparison.png`  \n",
    "    Grafik perbandingan metrik antar model.\n",
    "\n",
    "- **Error Analysis**\n",
    "  - `data/eval/error_cases_cosine.json`  \n",
    "    Daftar query yang gagal ditemukan dengan benar oleh pendekatan cosine similarity.\n",
    "  - `data/eval/error_cases_svm.json`  \n",
    "    Daftar query yang gagal diprediksi oleh pendekatan SVM.\n",
    "\n",
    "---\n",
    "\n",
    "Tahap ini menjadi penentu efektivitas sistem CBR dalam konteks hukum perdata (PMH), dan menjadi dasar pemilihan model untuk deployment dan pengembangan lanjutan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8e2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 5 - Evaluasi Model selesai\n",
      "- Hasil evaluasi retrieval disimpan di: data/eval/retrieval_metrics.csv\n",
      "- Hasil evaluasi prediksi disimpan di : data/eval/prediction_metrics.csv\n",
      "- Visualisasi disimpan di             : data/eval/performance_comparison.png\n",
      "- Error cosine disimpan di           : data/eval/error_cases_cosine.json\n",
      "- Error svm disimpan di              : data/eval/error_cases_svm.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from typing import Dict\n",
    "\n",
    "# === PATH SETUP ===\n",
    "COSINE_PRED = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED = \"data/results/predictions_svm.csv\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "RETRIEVAL_METRIC_OUTPUT = \"data/eval/retrieval_metrics.csv\"\n",
    "PREDICTION_METRIC_OUTPUT = \"data/eval/prediction_metrics.csv\"\n",
    "ERROR_COSINE_OUTPUT = \"data/eval/error_cases_cosine.json\"\n",
    "ERROR_SVM_OUTPUT = \"data/eval/error_cases_svm.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD GROUND TRUTH ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "    gt_dict = {i + 1: str(item[\"ground_truth\"]).strip() for i, item in enumerate(ground_truth_data)}\n",
    "\n",
    "# === EVALUASI RETRIEVAL COSINE (TOP-K) ===\n",
    "def eval_retrieval(pred_file: str, model_name: str, k: int = 5) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "\n",
    "        try:\n",
    "            pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "            if not isinstance(pred_ids, list):\n",
    "                pred_ids = []\n",
    "            pred_ids = [str(i).strip() for i in pred_ids]\n",
    "        except Exception:\n",
    "            pred_ids = []\n",
    "\n",
    "        hit = gt in pred_ids[:k]\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === EVALUASI SVM PREDICTION (TOP-1) ===\n",
    "def eval_prediction(pred_file: str, model_name: str) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "        pred = str(row.get(\"top_1_case_id\", \"\")).strip()\n",
    "\n",
    "        if not pred:\n",
    "            continue\n",
    "\n",
    "        hit = pred == gt\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    if not y_true:\n",
    "        print(f\"[PERINGATAN] Tidak ada prediksi valid untuk model {model_name}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1_score\": 0.0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === SIMPAN KASUS GAGAL ===\n",
    "def save_errors(pred_file: str, output_file: str, top_k: int = 5, mode: str = 'cosine'):\n",
    "    df = pd.read_csv(pred_file)\n",
    "    error_cases = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "\n",
    "        if mode == 'cosine':\n",
    "            try:\n",
    "                pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "                if not isinstance(pred_ids, list):\n",
    "                    pred_ids = []\n",
    "                pred_ids = [str(i).strip() for i in pred_ids]\n",
    "            except Exception:\n",
    "                pred_ids = []\n",
    "            hit = gt in pred_ids[:top_k]\n",
    "        else:\n",
    "            pred_ids = [str(row.get(\"top_1_case_id\", \"\")).strip()]\n",
    "            hit = gt == pred_ids[0]\n",
    "\n",
    "        if not hit:\n",
    "            error_cases.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"predicted\": pred_ids,\n",
    "                \"ground_truth\": gt\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === EKSEKUSI & SIMPAN HASIL ===\n",
    "retrieval_metrics = eval_retrieval(COSINE_PRED, \"TF-IDF + Cosine\")\n",
    "prediction_metrics = eval_prediction(SVM_PRED, \"TF-IDF + SVM\")\n",
    "\n",
    "pd.DataFrame([retrieval_metrics]).to_csv(RETRIEVAL_METRIC_OUTPUT, index=False)\n",
    "pd.DataFrame([prediction_metrics]).to_csv(PREDICTION_METRIC_OUTPUT, index=False)\n",
    "\n",
    "save_errors(COSINE_PRED, ERROR_COSINE_OUTPUT, mode='cosine')\n",
    "save_errors(SVM_PRED, ERROR_SVM_OUTPUT, mode='svm')\n",
    "\n",
    "# === VISUALISASI ===\n",
    "combined_df = pd.DataFrame([retrieval_metrics, prediction_metrics])\n",
    "if not combined_df.empty:\n",
    "    combined_df.set_index(\"model\")[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(\n",
    "        kind=\"bar\", figsize=(10, 6), title=\"Perbandingan Model\"\n",
    "    )\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data/eval/performance_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"[SUKSES] Tahap 5 - Evaluasi Model selesai\")\n",
    "print(f\"- Hasil evaluasi retrieval disimpan di: {RETRIEVAL_METRIC_OUTPUT}\")\n",
    "print(f\"- Hasil evaluasi prediksi disimpan di : {PREDICTION_METRIC_OUTPUT}\")\n",
    "print(f\"- Visualisasi disimpan di             : data/eval/performance_comparison.png\")\n",
    "print(f\"- Error cosine disimpan di           : {ERROR_COSINE_OUTPUT}\")\n",
    "print(f\"- Error svm disimpan di              : {ERROR_SVM_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170d3a7",
   "metadata": {},
   "source": [
    "### Kesimpulan\n",
    "\n",
    "---\n",
    "\n",
    "### Penutup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
